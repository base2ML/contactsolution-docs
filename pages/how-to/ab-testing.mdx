# A/B Testing

Test different placement designs and landing pages to optimize engagement and conversions.

import { Callout } from 'nextra/components'

<Callout type="info">
  A/B Testing is available on Pro tier.
</Callout>

<Callout type="warning">
  **Desktop Recommended:** Creating and managing A/B tests is best done on a desktop browser. Mobile devices can view test results, but for setup, we recommend switching to desktop for the optimal experience.
</Callout>

## Tier Limits

| Tier | Concurrent Tests | Test History |
|------|------------------|--------------|
| **Free** | Not available | — |
| **Pro** | Unlimited | 365 days |

## Overview

A/B testing allows you to create multiple versions of your placement landing pages and card designs, measuring which performs better. Track metrics like scan rates, engagement, and conversions to make data-driven decisions.

## Creating an A/B Test

1. Navigate to **Dashboard** → **A/B Testing**
2. Click **Create New Test**
3. Configure your test variants:
   - **Control (A)**: Your current design
   - **Variant (B)**: The design you want to test
4. Set your test parameters:
   - Test duration
   - Traffic split (default: 50/50)
   - Primary metric to optimize

## Test Configuration Options

| Setting | Description |
|---------|-------------|
| **Traffic Split** | Percentage of visitors seeing each variant (50/50 or custom) |
| **Test Duration** | How long to run the test (minimum 7 days recommended) |
| **Primary Metric** | The main goal: scans, engagement, or conversions |
| **Confidence Level** | Statistical significance threshold (default: 95%) |

## What You Can Test

### Design Elements
- Profile photo vs. logo
- Color schemes
- Layout arrangement
- Call-to-action text

### Content
- Headline variations
- Contact field order
- Social link placement
- Bio descriptions

### Functionality
- Landing page design
- Wallet pass design
- vCard information included

## Viewing Results

The A/B Testing dashboard shows:

- **Winner Badge**: Which variant is performing better
- **Scan Rate**: How often each variant is scanned
- **Engagement Score**: Combined performance metric
- **Statistical Significance**: Whether results are reliable

## Best Practices

1. **Test one thing at a time** - Isolate variables for clear results
2. **Run tests for at least 7 days** - Account for day-of-week variations
3. **Get enough traffic** - Minimum 100 scans per variant recommended
4. **Document your hypotheses** - Know why you're testing what you test

## Implementing Winners

When a test reaches statistical significance:

1. Click **Apply Winner** to make the winning variant your default
2. Or click **Start New Test** to iterate further
3. All data is preserved in your test history

## Related

- [Analytics Guide](/how-to/analytics) - Understanding your metrics
- [Pro Features](/features/pro) - Unlock A/B testing
